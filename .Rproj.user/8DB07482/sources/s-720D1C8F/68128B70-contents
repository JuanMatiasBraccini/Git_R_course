# Statistical and Graphical Analysis in R                     October, 2011 

# Instructor:	Janos (John) M. Hoenig 
# Contact:	hoenig@vims.edu  

# A brief introduction to linear models in R.
# see also the handy reference to useful functions at the end of this script.
#     see my script called formulae in R.r in the folder called reference 
#     documents & scripts for a detailed description of how to specify formulae.
################################################################################
####### generate some data for a linear regression
set.seed(3249)    # This sets the starting point for the random number 
                  # generators. If you execute this line you will get exactly
                  # the same outputs I got. If you don't execute this line,
                  # you'll generate a different set of random numbers and your
                  # results won't match mine exactly.

x1 = rnorm(40,20,4)      # generate values of independent variable 1
x2 = x1 + rnorm(40,30,4) # generate 2nd indep. variable. Note x1 & x2 correlated
b0 = -5                  # true intercept
b1 = 1.5                 # true coeff. (slope) for x1
b2 = -.1                 # true coeff. for x2
y = b0 + b1*x1 + b2*x2 + rnorm(40,0,4)    # dependent variable
sex = as.factor(rep(c("m","f"),c(20,20)))
sex
thedata = cbind(x1,x2,sex)

########### look at the data
plot(x1,x2,pch=as.character(sex))
plot(x1,y,pch=as.character(sex))
plot(x2,y,pch=as.character(sex))

cor(cbind(y,thedata))  # look at the correlations of y with the explanatory
                       # variables and also the correlation among the
                       # explanatory variables

# another thing we can do is look at partial correlations: Compute a matrix of 
# partial correlations between each pair of variables controlling for the others 
library(Rcmdr)   # you will need to have downloaded this package
partial.cor(cbind(y,thedata)) # We can see the correlation between y and x1
                              # after controlling for x2 and sex is 0.76;
                              # the correlation between y and x2 after 
                              # controlling for x1 and sex is -0.42
                            
################################################################################
##########   Example 1: simple linear regression

plot(x1,y,ylim=c(0,40),xlim=c(0,30)) #plot the data -leave graphics window open
first.example = lm(y~x1) # fit linear regression, store results in first.example
summary(first.example)     # look at output
abline(first.example)      # add the regression line

# look at graphs produced by plot()
par(mfrow=c(2,2))
plot(first.example)  # note: if we want those 4 graphs on separate pages, then
                     # simply omit the par(mfrow=c(2,2)) statement. R will 
                     # draw the first graph. Then simply click on the graphics
                     # window and it will draw the second graph, and so on.
par(mfrow=c(1,1)) # this just resets par so you'll get 1 graph per page (so you
                  # don't have to close the graphics window to reset par)
                  
# we can make these plots manually and have more control over the output.

# plot the residuals to look for lack of fit (trend) & nonconstant variance
plot(fitted(first.example),residuals(first.example),typ="h") # typ="h" plots
                                                             # distances from 0
abline(h=0)     # add horizontal line at 0

# now make a Q-Q plot of the residuals to check for normality
qqnorm(residuals(first.example),ylab="Residuals")
qqline(residuals(first.example)) # this adds a line joining the first and third
                                 # quartiles
                                 
hist(residuals(first.example))  # a histogram of residuals is not a great way to
                                # check for normality because the results depend
                                # on how the data are binned

confint(first.example)     # confidence intervals for the slope & intercept
# plot a joint confidence region for the intercept and slope (you need the
# ellipse package)
library(ellipse)
plot(ellipse(first.example,c(1,2)),typ="l",xlab=expression(beta[0]),
   ylab=expression(beta[1]),
   main="95% joint confidence region for intercept and slope")
points(coef(first.example)[1],coef(first.example)[2],pch=18) #add point estimate
abline(v=confint(first.example)[1,],lty=2)  # add 1-way conf. int. for intercept
abline(h=confint(first.example)[2,],lty=2)  # add 1-way conf. int. for slope

# get the anova table for the regression
anova(first.example)

##########   Example 2: now force the regression to pass through the origin
second.example = lm(y ~ x1 - 1)    # the minus one means "no intercept"
summary(second.example)
plot(x1,y,ylim=c(-10,40),xlim=c(0,30))
abline(second.example)

par(mfrow=c(2,2))
plot(second.example) # Look for patterns to the residuals
par(mfrow=c(1,1))

##########   Example 3: multiple linear regression
# first regress y on x1 and on x2 (separately)
summary(lm(y~x1))
summary(lm(y~x2))
# quite clearly, x1 is a better predictor than x2. Does x2 help if we already
# have x1?

summary(lm(y~x1 + x2))  # note the syntax: we signify main effects for x1 and
                        # x2 by x1 + x2     
# both variables are significant

model1 = lm(y~x1)       # store the results of the first model
model12 = lm(y~x1+x2)   # store the results of the second model
anova(model1,model12)    # list null (reduced) model first.
# We see that we reject the null hypothesis that the reduced model is
# sufficient in favor of the alternative that we also need x2

# partial regression (added variable) plot: We plot regress y on all predictors
# except one and obtain the residuals. These represent y after removing the
# effect of all variables except the one excluded. Call this d. We also regress
# the excluded variable on all other predictors and obtain the residuals. These
# represent the effect of the excluded variable after removing the effect of the
# other predictors. Call these residuals m. We then plot d versus m.

d = residuals(lm(y~x1))   # residuals from model leaving out x2
m = residuals(lm(x2~x1))  # regression of excluded variable (x2) on the
                          # other variable(s)
plot(m,d,xlab="x2 residuals",ylab="y residuals")
title("added variable plot")

# interpretation:
coef(lm(d~m))     # note the slope
coef(model12)     # note the slope from the full model is the same as the slope
                  #    in the added variable plot
abline(lm(d~m))
# We use the plot to look for nonlinearity and/or outliers or influential points
# (and don't see any problems)

# partial residual plots. Here, we plot the residual from the full model plus
# the effect of one variable versus the value of the variable. That is, we can
# plot residual + betahat1 * x1 versus x1 where betahat1 is the regression
# coefficient for the variable x1 from the full model. 

plot(x2,residuals(model12) + coef(model12)['x2']*x2,xlab="x2",
   ylab="y (adjusted)",main="partial residual plot")
abline(0,coef(model12)["x2"])
# Note: partial residual plots can be made using the termplot() function
termplot(model12,partial.resid=T,rug=T,se=T)

# According to Faraway (Linear Models with R, Chapman & Hall, 2005, page 72),
# added variable plots are supposed to be better for detecting outliers and 
# influential data points and partial residual plots are supposed to be better 
# for detecting nonlinearity.

# Fitting with an OFFSET: suppose that for theoretical reasons we believe the
# coefficient for x2 should be -0.15 and we want to include a term -.15 x2 in
# the model. This is called an offset and it is specified as follows:

lm(y ~ x1 + offset(-.15 * x2))

# Fitting models with INTERACTIONS. To specify a model with an interaction
# between x1 and x2, corresponding to the model
#      y = b0 + b1 x1 + b2 x2 + b3 x1 x2 + error
# we use the colon operator (:)

fullmod = lm(y ~ x1 + x2 + x1:x2)
summary(fullmod)
# NOte that now nothing is significant except for the intercept. This does not
# mean that nothing is significant! We should eliminate the interaction term
# and then we'll get significant results. An easy way to do this is with the
# update() function

no.interact = update(fullmod, . ~ . - x1:x2)
# in the above, . means everything as in full mod, and the -x1:x2 means "but
# remove the interaction between x1 and x2"
summary(no.interact)    # now x1 and x2 are significant
anova(no.interact,fullmod)  # anova to compare model without (null) and with
                            # interaction. Model with interaction is not
                            # significant, which makes sense because we
                            # created the data without an interaction.

# Transformations within a call to lm using I().
# Suppose we want to regress Y on the sum of x1 and x2. We could create a 
# variable sumx = x1 + x2 and then call lm(). However, you can do that within
# the call to lm() as follows:

lm(y ~ I(x1 + x2))

# The identity function I() tells R to do whatever needs to be done within the
# parentheses and then use the result as a variable in the regression. Without
# the parentheses R will think you want x1 to be a variable and x2 to be a 
# variable, each with separate regression coefficients.

##########  Example 4: fit a linear regression with INDICATOR (DUMMY) variables.
#  An indicator variable takes on either the value of 1 or 0 depending if a
#  trait exists or does not exist. For example, we could have sex = 1 if the
#  animal is female and sex = 0 if it's not female. Linear regression with
#  dummy variables is equivalent to analysis of covariance (ancova). However,
#  with ancova we are usually interested in comparing means of groups after 
#  adjusting for covariates whereas in regression we may be interested in the
#  effects of the covariates as well as the effects of the categories defined
#  by the dummy variables. 

x = rnorm(20,10,2)
y = 3 + .5*x + rnorm(20,0,1)
sexfac = as.factor(c(rep("m",10),rep("f",10))) # designate the first 10
                                               # observations as male & the last
                                               # 10 as female
y[11:20] = y[11:20] + 15   # make females have bigger y values than males
xlimits = c(0,20)
ylimits = c(0,40)
plot(x[sexfac=="m"],y[sexfac=="m"],xlim=xlimits,ylim=ylimits,xlab="x",ylab="y")
males = lm(y[sexfac=="m"]~x[sexfac=="m"])
abline(males) # solid line
par(new=T)
plot(x[sexfac=="f"],y[sexfac=="f"],xlim=xlimits,ylim=ylimits,xlab="x",ylab="y",
   pch=2)
females = lm(y[sexfac=="f"]~x[sexfac=="f"])
abline(females,lty=2)  # dashed line
legend(2,38,c("separate female regression","separate male regression",
   "regression with dummy"),lty=c(2,1,3))

# we see the two regression lines are roughly parallel - same slope but
# different intercepts. So, we can fit a model with one slope and two
# intercepts instead of two slopes and two intercepts, thus reducing the
# number of parameters we have to estimate from our data
dummy = lm(y ~ x + sexfac)  # Note that although sexfac is a factor with levels
                            # f and m, lm treats sexfac as having values 0 and 1
summary(dummy)
     # summary() tells you the intercept for females is around 18, and y 
     # increases around 0.5 for every unit change in x; furthermore, the
     # slope for males is the same as for females but the intercept is about
     # 3. The intercept for males is found by adding the estimated sex effect
     # (-15) to the intercept (18). (The exact values will vary from run to run 
     #  because the data are generated randomly each run.)
abline(dummy$coefficients[1],dummy$coefficients[2],lty=3)
abline((dummy$coefficients[1]+dummy$coefficients[3]),dummy$coefficients[2],lty=3)     

################################################################################
# ANOVA

# Below we examine one-way anova and two-way anova for balanced designs
###############################
# generate data for 1-way anova
ydat = rnorm(100,0,5)          # generate 100 random numbers
# create the treatment effects and add them to the random errors (ydat) to
# create the observations
add = c(rep(0,20),rep(5,20),rep(1,20),rep(7,20),rep(12,20)) 
add   # this is just a string of 20 zeros, 20 fives, 20 ones, etc. representing
      # the means for each of the 5 treatments
ydat = ydat + add   # this is the treatment mean plus a random error
nutrients = rep(1:5,rep(20,5))   # create a vector to indicate the treatment for
                                 # each observation in ydat.
                                 # (The above is just a fancy way to create a
                                 # vector of 20 ones, 20 twos, 20 threes, 20
                                 # fours and 20 fives)
nutrients 

##########  Example 5: do the one-way anova                     
example5 = lm(ydat~nutrients)# SEEMS to work, BUT...
summary(example5)    # *****problem*****: lm() is treating nutrients as a 
                     # quantitative variable rather than as a label for the 
                     # treatment level. That's why the output has an intercept 
                     # and slope instead of treatments. It's doing a linear
                     # regression.
                     # We need to make nutrients a vector of FACTOR levels. Then
                     # lm() will do a one-way anova.
is.factor(nutrients) # nutrients is not a factor
nutrients = as.factor(nutrients)   # "coerce" (convert) to factor levels
nutrients             # note the levels appearing below the listing
is.factor(nutrients)  # yes, nutrients is now a factor vector
example5 = lm(ydat~nutrients)
example5            # Ah! Now we're seeing treatment levels
summary(example5)   # OK, we have our estimates. But what are we looking at?
   tapply(ydat,nutrients,mean)  # this gives the mean of the ydat values for
                                # each level of nutrients
   # So, here's what's happening: summary(example3) gives you the mean for
   # treatment 1 (which it calls "(intercept)"). It also gives you the 
   # TREATMENT EFFECTS for the other treatments, which are the
   # amounts by which the treatment mean differs from the mean of treatment 1.
   # That is, the mean of treatment 2 is equal to the mean of treatment 1 plus 
   # the treatment effect for treatment 2.
par(mfrow=c(2,2))   
plot(example5)    # no pattern to the residuals; the normal Q-Q plot looks quite
                  # linear (slight departures at the ends, notwithstanding)
par(mfrow=c(1,1))
anova(example5)     # get the anova table

# multiple comparisons using Tukey's Honest Significant Differences
example5a = aov(ydat~nutrients)   # need output from aov(), not lm()
TukeyHSD(example5a)               # tabular results
plot(TukeyHSD(example5a))         # graphical results

### Note: from the summary() output we saw that the estimate for nutrient level
### 3 was not statistically significant. We KNOW that in fact the data for
### nutrient level 3 were drawn from a population with a mean of 1 whereas the
### data for nutrient level 1 were drawn from a population with a mean of 0
### i.e., treatments 1 and 3 ARE different). But, the difference in means is so
### slight that we wouldn't be surprised to see nonsiqnificant results. Hence,
### as data analysts we might want to consider combining treatments 1 and 3. We
### do that below.
nutrientsmod = nutrients # make a copy of the nutrients factor
levels(nutrientsmod)[c(1,3)] = "control"  # make the label for both treatments
                                          # 1 and 3 be "control"
nutrientsmod
example6 = lm(ydat~nutrientsmod)
anova(example6,example5)   # this compares the fits of the two models. The p
                            # value for Model 2 (with the additional factor 
                            # level) is large suggesting there is little support 
                            # including an effect for nutrient level 3
                            
                            # Note that in doing the comparison, we specify the
                            # model with fewer parameters first.
###############################
# now generate data for 2-way anova by creating vector of temperature treatments
temp = rep(c("low","high"),10)
temp    # note: this is a character vector, not a factor vector
temperature = as.factor(rep(temp,rep(5,20)))
temperature
# NOTE: we are not modifying the data according to temperature so we'd hope and
# expect that temperature does not turn out significant.

##########  Example 6: factorial design: do a 2-way analysis of variance 
#                      without interactions
example7 = lm(ydat ~ nutrients + temperature)
summary(example7) # Again, it worked, but what are we looking at?

   # summary() gives the fitted (predicted) mean for high temperature and 
   # nutrient level 1 (labelled (intercept)). To get the prediction for nutrient
   # level 2 and high temperature add the estimate for nutrients2 to the
   # intercept, and do similarly for nutrient levels 3, 4 and 5. To get
   # estimates for low temperature, subtract the estimate labelled 
   # temperaturelow from the corresponding estimate for high temperature.
   
confint(example7) # get confidence intervals for the estimated parameters

anova(example7)   # As expected, nutrient level is highly signficant and 
                  # temperature is not significant

# we might like to compare the fit of the model with and without temperature.
# We can do that using anova() as follows:
anova(example5,example7)  # we see that example7 (labelled Model 2) shows 
                          # almost no improvement over example5 (labelled
                          # Model 1) so we have extremely little justification
                          # for including temperature in the model.

# multiple comparisons
example7a = aov(ydat ~ nutrients + temperature) # do the same analysis using aov
TukeyHSD(example7a)               # tabular results
par(mfrow=c(2,1))
plot(TukeyHSD(example7a))         # graphical results
###############################
# example 7: 2-way anova with interaction 
# (note how interactions are specified with : )
example8 = lm(ydat~ nutrients + temperature + nutrients:temperature)
# the above can also be specified more briefly as:
example9 = lm(ydat ~ nutrients*temperature)
summary(example8)  # not surprisingly, the estimate for the effect of
                   # temperature and the estimates of the interactions are not
                   # statistically significant, at least when I ran the analysis
summary(example9)     # exactly the same as example8

anova(example8)    # only nutrients are statistically significant

anova(example5,example7,example8)   # here we compare all 3 models at once.
                                    # Neither Model 2 nor Model 3 (example7 and
                                    # example8, respectively) are a signficant
                                    # improvement over the simpler model with
                                    # just nutrient effects
                                    
################################################################################
#
# Useful functions for linear models

?add1           # Add or Drop All Possible Single Terms to a Model
?aov            # alternative to lm()
?anova          # create an anova table for a fitted model (i.e., fit the model
                # first, then call anova()
?bartlett.test  # test for non-homogeneous variances (or use xxx)
?coef           # extract the parameter estimates
?confint        # Confidence Intervals for Model Parameters
?drop1          # Drop All Possible Single Terms to a Model
?extractAIC     # Extract AIC from a Fitted Model (see also step() below)
?fitted
?lm             # Fit Linear Models
?manova         # Multivariate Analysis of Variance
?model.matrix   # show the design matrix (X matrix) used by R to fit the linear model
?oneway.test    # one-way anova without assumption of homogeneous variances,
                # using Welch's method
?pairwise.t.test# Calculate pairwise comparisons between group levels with 
                # corrections for multiple testing 
?poly           # fit models with a polynomial function of x
?predict
?replications   # Returns a vector or a list of the number of replicates for 
                # each term in the formula
?residuals
?step           # Choose a model by AIC in a Stepwise Algorithm
?termplot       # Plots regression terms against their predictors, optionally 
                # with standard errors and partial residuals added. Use to
                # assess the added contribution of a variable to a model. Also
                # called partial residual plots
?TukeyHSD       # Compute Tukey Honest Significant Differences, i.e., do
                # multiple comparisons. This is meant for balanced or mildly
                # unbalanced data. You apply it to an object generated by 
                # aov(), NOT lm()
?update         # Use this to remove one or more terms from a linear model that
                #   you've already fitted and saved

# The multcomp library provides a variety of multiple comparison procedures and
# procedures for testing linear hypotheses about treatment effects.

# note: many generic functions like summary, predict, fitted and residuals can 
# be used with linear models