---
title: "Lesson 4. General statistics ...continued"
knit: (function(inputFile, encoding) {
  rmarkdown::render(inputFile, encoding = encoding, output_dir = "C:/Matias/Cursos/2019_Mexico/htmls") })

output: 
  html_document:
    code_folding: show
    highlight: haddock
    keep_md: yes
    theme: united
---

<style type="text/css">
h1.title {
  font-size: 30px;
    color: Maroon;
  text-align: center;
          }
h3.subtitle {
  font-size: 22px;
    color: Maroon;
  text-align: center;
            }
h4.author { 
    font-size: 24px;
      color: FireBrick;
  text-align: center;
          }
h4.date { 
  font-size: 18px;
  text-align: center;
        }
h1{
  font-size: 22px;
  color: DarkBlue;
  }
h2{
  font-size: 20px;
  color: Blue;
  }
h3{
  font-size: 18px;
  color: SteelBlue;
  }
body{
    font-family: Helvetica;
    font-size: 14pt;
    }
code.r{
  font-size: 16pt;
      }
pre {
  font-size: 16pt;
}
</style>


```{r globaloptions, include=FALSE}
knitr::opts_chunk$set(fig.width = 6,fig.height = 6,
                        echo = TRUE, warning=FALSE,message=FALSE)
```
# Lesson goals
* We're going to learn how to implement:
    + generalised linear models
    + model optimisation through maximum likelihood estimation
    + bootstrapping and Monte Carlo methods
 
<br>
Credits to John Hoenig, Ainslie Denham and Alex Hesp
<br><br>

# Before starting...some further R tips
```{r}
# remove anything stored in memory
rm(list=ls())

# check your working directory
getwd()

# change your working directory
setwd("C:/Users/myb/Desktop")
setwd("C:\\Users\\myb\\Desktop")  #same thing...
getwd()

# The default settings sets columns with words to type 'factor'.
options(stringsAsFactors = FALSE)  #telling R not to do that...

```
<br><br>

# Generalised linear models

## Logistic regression of proportion mature versus length of hammerhead sharks.

A linear regression would not be a good, general solution because it could predict a negative proportion or a proportion greater than 1, neither of which makes sense. 
Thus, we want our response curve to look “S”-shaped. 
The logistic model is: 
$$P(Y=1)=\frac{e^{\beta_0+\beta_1x}}{1+e^{\beta_0+\beta_1x}}$$
where Y is the response, x is the explanatory variable, and $\beta_0$ and $\beta_1$ are the regression coefficients to be determined. 

Here, Y is a dichotomous variable taking on the values 1 if a condition occurs and 0 if it does not occur. 

As an example, consider data on male scalloped hammerhead maturity versus length – see graph below.


```{r}
# note: data and analysis by Al Harry (Harry et al 2011) and
#       modified by J. Hoenig
#       lengths measured to the nearest mm

hndl='C:/Matias/Cursos/2019_Mexico/Data.sets/'
hammerdata<-read.table(paste(hndl,"Lesson4_HammerheadMaturity.txt",
                             sep=''),header=T)
dim(hammerdata)        # number of rows and columns in the data
str(hammerdata)        # structure of the database
head(hammerdata,n=4)   # first 4 rows
summary(hammerdata)    # note: ALL are males

# extract the variables into more convenient names
Stage = hammerdata$Stage
STL = hammerdata$STL

plot(STL,Stage)

# To specify a logistic regression it suffices to say 
# family=binomial (for the error structure) because the 
# default for family=binomial is to use a logit transform
# which gives the logistic model. 

#fit model
Model1 <- glm(Stage~STL,family=binomial)
# the function glm() fits a generalized linear model
# Stage~STL is a formula which means that the maturity 
# stage (immature or mature) is explained by the length
# of the shark.
# By specifying family=binomial we are saying we want to fit
# a logistic relationship of proportion mature versus length STL
   
summary(Model1)
class(Model1)
# the output type is of class "glm" (also class "lm")
# R has "generic" functions, like plot( ) and summary( ),
# which recognize various classes of object and automatically
# use the correct "method" for operating on that class 

attributes(Model1)  # what's stored in Model1
Model1$aic    # you can access this information using the $ notation

  
# Make some graphs
plot(Model1)
# What's happening here? 
# plot( ) is recognizing the output is of class "glm" and
# is choosing the plotting routines to graph the results.

# Unfortunately, we have binary data so the results are not
# meaningful.
# (See below for an explanation of binary and binomial data)


# If we want to see the fitted logistic curve we can compute
#  predicted values for a vector of x-values (lengths, STL)

#First create a vector with 50 lengths for which we want predictions
predictionpoints = seq(min(STL),max(STL),length.out=50)
predictionpoints

#Next get predictions from the model for that vector
predictions = predict(Model1,newdata=list(STL=predictionpoints),
                      type="r")

# NOTE: there are two tricks here. 

# First, although the help page for predict doesn't tell you, 
# you must specify the values of x for which you want predictions
# as a list.
# We specify STL=predictionpoints so that when R looks at Model1 
# and expects to see the x-variables be called STL it will know to
# substitute predictionpoints for STL.

# Next, note that we used the glm() function which transforms the
# data to a linear relationship. 
# When we ask for predictions we need to specify on which scale 
# we want the predictions - on the transformed (linear) scale or
# on the actual scale of the data. 
# Here, we specified type="response" to get
# the predictions on the original scale. 
# The default is type="link" which would give the predictions on
# the linearized (logit) scale.

plot(predictionpoints,predictions,xlab="",ylab="Proportion mature",
     xlim=c(0,2500))

#manual construction
bo=-24.19322975
b1=0.01644139
y=exp(bo+b1*predictionpoints)/(1+exp(bo+b1*predictionpoints))
lines(predictionpoints,y,col=2)
legend('topleft',c('predict_preds','manual_preds'),text.col=1:2)
```
```{r}
# Let's look at the parameter estimates
Model1$coefficients

# compute the 95% confidence intervals using the profile
# likelihood method
confint(Model1)


# Show the length where 50% are mature
# We could solve the logistic equation for L50:
# .5 = 1/(1+exp(bo + b1*L))
# where bo is the intercept and b1 is the slope estimated
# by the logistic regression. Thus, 
#       Length at 50% maturity = -bo/b1
leng50 = -Model1$coefficients[1]/Model1$coefficients[2]
leng50

# but...someone has already created a function to do it. 
# ... Suppose we wanted to know the lengths
# at which 50% and 90% of the sharks are mature
library(MASS)
dose.p(Model1,p=c(0.5,0.9))


# Add 95% CI to predicted values
predictions.SE = predict(Model1,newdata=list(STL=predictionpoints),
                      type="r",se.fit = TRUE)
Upper.95=predictions+1.96*predictions.SE$se.fit
Lower.95=predictions-1.96*predictions.SE$se.fit

par(mfcol=c(3,1),mar=c(4,4,.1,.1),oma=c(2,2,.1,.1),mgp=c(2,.8,0),cex.lab=1.5)
plot(predictionpoints,predictions,xlab="",ylab="Proportion mature",
     xlim=c(0,2500),ylim=c(0,1),type='l')
lines(predictionpoints,Upper.95,col=2)
lines(predictionpoints,Lower.95,col=2)

plot(predictionpoints,predictions,xlab="",ylab="Proportion mature",
     xlim=c(0,2500),ylim=c(0,1),type='l',col=2)
polygon(c(predictionpoints,rev(predictionpoints)),
        c(Lower.95,rev(Upper.95)),col=rgb(1,.1,.1,alpha=.2))


plot(predictionpoints,predictions,xlab="Length",
     ylab="Proportion mature",xlim=c(0,2500),ylim=c(0,1),pch=19)
segments(predictionpoints,Upper.95,predictionpoints,Lower.95)


```

```{r}
# Binomial (binned) data:
# Instead of having exact lengths for each shark
  #  we have the length recorded by interval,
  # such as 5 cm "bins". 
# Then, the proportion mature in each bin or
  # interval is a binomial random variable. 
# note: If we have binomial data, we might worry about 
#       overdispersion, which is when the data have more
#       variability than what is theoretically supposed 
#       to occur. For examples, if shark samples come from
#       shark schools of similar size...
#       In such a case, test for overdispersion by 
#       estimating the dispersion parameter (i.e.
#       divide the residual deviance by the residual 
#       degrees of freedom. If this is > 1 use a
#       quasilikelihood  (See below).

#How to bin your data
binned.data = table(cut(STL,15),Stage)
#cut() is assigning each length to one of 15 bins
#     and each maturity to one of 2 bins (immature and mature).
#table() counts how many observations are in each
# combination of length x maturity.
binned.data    

#plot proportion mature versus the midpoint of the length bins
proportion.mature = binned.data[,2]/
                    (binned.data[,1] + binned.data[,2])
range(STL)
binwidth = (max(STL) - min(STL))/15
binwidth
bins = 0:14
midpoints = min(STL) + .5*binwidth + binwidth*bins
midpoints
plot(midpoints,proportion.mature,ylab="proportion mature",
     xlab="length",xlim=c(0,2500))

#fit model to binned data
y = cbind(binned.data[,2],binned.data[,1])
y
Model2 = glm(y~midpoints,family=binomial)
Model2
summary(Model2)
# The overdispersion parameter can be computed from the output as
# residual deviance / degrees of freedom = 1.7865/13, which is 
# < 1 by far so we would conclude we have no evidence
# of overdispesion. But, for illustrative purposes we try
# quasilikelihood.
Model3 <- glm(y~midpoints,family=quasibinomial)
Model3


predictions2 = predict(Model2,type="r")
# NOTE: Because the data are binned, we can only 
# compute predicted values for the 15 length bins.
# Also, we use the same trick described above and tell R 
# that we want the predictions on the original scale 
# (proportion mature) rather than on the transformed (logit) scale.

plot(predictionpoints,predictions,xlab="",ylab="Proportion mature",
     type='l',xlim=c(0,2500))
lines(midpoints,predictions2,col=2)
legend('topleft',c("binary data","binomial"),lty=1,col=1:2,bty='n')

```
<br><br>

## Wrap up exercise
1. Read in "Lesson4_exercise1.csv" 
2. Fit logistic model
3. Extract summary, and % deviance explained (this is (null.deviance-model deviance)/null.deviance
4. Get model predictions for the size range 100:200
5. Plot predictions and 95% confidence intervals (as a blue filled polygon)

<br><br>


## Model selection
There are cases where we have multiple *potential* predictors (i.e. model terms) and we want to find out which of those are effecting our response variable.

Consider our previous case, the maturity at size of Hammerheads, but now we have other potential predictors, such as sampling location (e.g. north and south), and sampling person (e.g. Tim and Betty). Our equation is now:

$$P(Y=1)=\frac{e^{\beta_0+\beta_1Size+\beta_2Location+\beta_3Sampler}}{1+e^{\beta_0+\beta_1Size+\beta_2Location+\beta_3Sampler}}$$
<br>
We are going to use a different case study

```{r}
#Read in data (use of contraceptive methods)
cuse=read.csv("C:/Matias/Cursos/2019_Mexico/Data.sets/Lesson4_cuse.csv")
head(cuse)

#Let's look at the effect of different potential predictors 
# on the use of contraceptive methods

#Some basic data exploration
par(mfrow=c(3,2),mar=c(3,3,.5,.1),mgp=c(1.9,.8,0))
boxplot(cuse$notUsing~cuse$age,ylab="Not using",
        xlab="Age",col="orange")
boxplot(cuse$using~cuse$age,ylab="Using",
        xlab="Age",col="orange")

boxplot(cuse$notUsing~cuse$education,ylab="Not using",
        xlab="Education",col="orange")
boxplot(cuse$using~cuse$education,ylab="Using",
        xlab="Education",col="orange")

boxplot(cuse$notUsing~cuse$wantsMore,ylab="Not using",
        xlab="WantsMore",col="orange")
boxplot(cuse$using~cuse$wantsMore,ylab="Using",
        xlab="WantsMore",col="orange")
```

```{r}
#Stepwise model selection (i.e. add one term at a time)
mod1 <- glm( cbind(using, notUsing) ~ age , 
             family = binomial, data=cuse)
mod2 <- glm( cbind(using, notUsing) ~ age + education ,
             family = binomial,data=cuse)
mod3 <- glm( cbind(using, notUsing) ~ age + education + wantsMore ,
             family = binomial,data=cuse)
mod4 <- glm( cbind(using, notUsing) ~ age * education + wantsMore ,
             family = binomial,data=cuse)
mod5 <- glm( cbind(using, notUsing) ~ age * wantsMore + education ,
             family = binomial,data=cuse)

#combine all models in a list
mods=list(mod1=mod1,mod2=mod2,mod3=mod3,mod4=mod4,mod5=mod5)

library(wiqid)

#AIC corrected for small sample size
AICc <- sapply(mods, AICc)
sort(AICc)

# Compare model AICS and weights
AICtable(AICc)


#Automated model selection: package 'glmulti' 
# source: https://www.jstatsoft.org/article/view/v034i12


```

<br><br>

# Model optimisation through maximum likelihood estimation

Wikipedia:

* In statistics, maximum likelihood estimation (MLE) is a method of estimating the parameters of a probability distribution by maximizing a likelihood function, so that under the assumed statistical model the observed data is most probable

* The point in the parameter space that maximizes the likelihood function is called the **maximum likelihood estimate**

* The logic of maximum likelihood is both intuitive and flexible, and as such the method has become a dominant means of statistical inference

<br>
We will continue using maturity data, in this case, of crabs from Western Australia, and will use the following equation:


$$P(mature)=\frac{1}{1+e^{(-\alpha(Length-L50))}}$$
where  $\alpha$  is the slope and L50 is the length where 50% of the population is mature


```{r}

#Bring in data
setwd('C:\\Matias\\Cursos\\2019_Mexico\\Data.sets')
MatDat <- read.csv("Lesson4_MaturityData.csv", header = T)
head(MatDat)

# so we have a unique fish number, the age and length of each fish,
# and a variable called ObsMatCat (short for 
# observed maturity category).

#Maturity is either 0 or 1 (i.e. 1, mature or 0, immature)
unique(MatDat$ObsMatCat)

# how many length obs?
length(MatDat$ObsLen)

# how many age obs?
length(MatDat$ObsAge)

# same, good!

# let's store this information
nobs <- length(MatDat$ObsLen)

# get minimum and maximum lengths
min(MatDat$ObsLen)
max(MatDat$ObsLen)

# get minimum and maximum ages
min(MatDat$ObsAge)
max(MatDat$ObsAge)

# specify minimum and maximum lengths for analysis, and interval
# for length categories
MinLen <- 0
MaxLen <- 600
LenInc <- 50

# set up vector of length bins using numbers specified above
(LenCats <- seq(MinLen,MaxLen,LenInc))

# determine number of length categories
(nlengthcats <- length(LenCats))

# get length category number for each observation
# note that fish between 0-49 mm are in the first length category
# explaining the "+1" bit in the calculation.
# Fish between 50-99 mm are in the second length category and so on 
ObsLenCats = trunc(MatDat$ObsLen/LenInc)+1
# let's do a check


# set up some vectors, so we can store the observed frequencies of
# mature and immature fish in each length bin
(ObsFreqImm = rep(0,nlengthcats))
(ObsFreqMat = rep(0,nlengthcats))

# sum up the numbers of fish with a maturity category of 0 (immature)
# in each 50 mm length class,
# Do the same for those with a maturity category of 1 (mature).
for (i in 1:nlengthcats)
{
  ObsFreqImm[i] <- length(which(ObsLenCats==i & MatDat$ObsMatCat==0))
  ObsFreqMat[i] <- length(which(ObsLenCats==i & MatDat$ObsMatCat==1))
}
ObsFreqImm
ObsFreqMat

# calculate proportion mature in each length category
(PropMat = ObsFreqMat / (ObsFreqMat + ObsFreqImm))

# A few NaN (not a number values) - this is due to
# attempting to divide by zero, i.e. dividing by a total sample size 
# of zero for certain small and very large length categories. 
# We'll deal with the NaNs later.
(ObsFreqMat + ObsFreqImm)

# plot trend
(xVals <- LenCats + (LenInc/2))  
# xvals defines the lower bounds of the length categories. 
# here we've added 25 mm to the values used to define the x axis so
# that the points go throughthe middle of the length categories 
# (noting that there are various lengths of fish in each
#  length caegory, so any statistic for the overall length category
# is best represented at the middle of this category on a plot.
# For example, for the 0-49 mm length category, the value for 
# proportion mature for the full category is plotted at 25

plot(xVals,PropMat,"o",xlab="length",ylab="prop mature",frame.plot=F, 
     xlim=c(0,MaxLen),ylim=c(0,1))

# the trend is roughly s-shaped, so it might be appropriate to fit 
# a logistic curve to these data. 

```

```{r}


# I'd like to know the sample size in each length category 
#   - for each length category,is the calculated proportion based
#     on a large or small sample
#   - how reliable are our values for proportion mature for each
#     length category?


# plot observed vs expected trends
nObsInLenCat <- ObsFreqImm + ObsFreqMat 

plot(xVals,PropMat,"o",xlab="length",ylab="prop mature",
     frame.plot=F,xlim=c(0,MaxLen),ylim=c(0,1))
text(xVals,PropMat+0.07,cex=0.8,pos=1,labels=nObsInLenCat,
     col="forestgreen")
# So we have very low sample sizes for the smallest length categories
# very good sample sizes for fish between about 250 and 450, and 
# a very low sample size for fish above 500 mm. 

# So most information is for fish of medium-sized fish.

# What's the uncertainty associated with each proportion mature value?
# Let's calculate some approximate 95% confidence limits 
# (accepting that the confidence limits are not well estimated at 
#  low sample sizes, or when the proportion is close to zero
#   or 1, with the formula I've used below)

# here are our length categories
LenCats

# estimated confidence intervals (based on normal approximation)
PropConfInt95 <- 1.96 * sqrt(PropMat * (1 - PropMat) / nObsInLenCat)
PropConfInt95


# Again, a few NaN (not a number values) - we can't divide 
# anything by zero, so where we have a sample size of zero,
#  we get a NaN

# a few length categories have a sample size of zero,
# so can't calculate confidence intervals for these. 
# Where probability is close to zero or 1, 
# confidence interval is not reliable - so let's
# focus on the others)

# Let's calculate the "confidence limits" (note, the 
# interval is the range between the mean and confidence limit)
PropMat_Low95CL <- PropMat - PropConfInt95
PropMat_Low95CL
# One of our confidence limits is negative - can't have 
#     negative proportion.
# let's set any negative values to zero.
PropMat_Low95CL <- replace(PropMat_Low95CL,
                           which(PropMat_Low95CL<0),0)
PropMat_Low95CL
#?replace
# Could also use an 'ifelse' statement
#PropMat_Low95CL <- ifelse(PropMat_Low95CL<0,0,PropMat_Low95CL)

PropMat_Up95CL <- PropMat + PropConfInt95; PropMat_Up95CL
 
# No issues 
# It is possible that with another data set, some upper
# limits for proportions could go above 1. 
# Just in case, let's make sure it's never possible for
# this to happen, by setting our upper limit to 1. 
#This is known  as "defensive coding" 
PropMat_Up95CL <- replace(PropMat_Up95CL,
                          which(PropMat_Up95CL>1),1)
PropMat_Up95CL

# Now, we still have some NaNs, which will be a problem
# when we go to plotting our confidence limits. 
# Also, we can't plot a confidence limit where the interval 
# is zero (as the arrow will be infintesimally small).
# So we'll find where anything that is not a NaN or is 
# above zero and just plot the confidence limits for these
Positions <- which(!is.na(PropConfInt95) & PropConfInt95 > 0) 

# plot it
plot(xVals,PropMat,"o",xlab="length",ylab="prop mature",
     frame.plot=F,xlim=c(0,MaxLen),ylim=c(0,1))
arrows(xVals[Positions],PropMat_Low95CL[Positions], 
       xVals[Positions], PropMat_Up95CL[Positions], 
       code=3, angle=90,length=0.02, col='black')

```
```{r}

# After all that, its clear we have lots of samples around the
# 50% maturity mark, so we should be able to get a pretty 
# good estimate for the parameter L50.
# We might be less certain about what's going on for 
# smaller and larger crabs

# Right, so we want to fit a curve to the data. 
# In this case, this logistic (s-shaped) curve:
```
$$P(mature)=\frac{1}{1+e^{(-\alpha(Length-L50))}}$$

```{r}
#  Note that this curve is "symmetric" - the shape of the lower half 
#   is the same as the upper half, just inverted. 
# This is an assumption! Looking at the data, yes, this looks
# kind of true, so OK, we'll stick with this curve.


#When using maximum likelihood estimation, we must specify 
# starting values for our unknown parameters.

#From the graph, L50 seems somewhere around 300,
# but the slope ? ....let's take a guess!
L50 <- 300
slope <- 0.5

# now estimate probability of maturity at those lengths, 
# using our logistic equation
ProbOfMaturity <- 1 / (1 + exp(-slope * (xVals - L50)))

# let's overlay the line on our plot
plot(xVals,PropMat,"o",xlab="length",ylab="prop mature",
     frame.plot=F,xlim=c(0,MaxLen),ylim=c(0,1))
arrows(xVals[Positions],PropMat_Low95CL[Positions], 
       xVals[Positions], PropMat_Up95CL[Positions], 
       code=3, angle=90,length=0.02, col='black')
lines(xVals,ProbOfMaturity, type="o",col="blue")
# right, so the slope is way too high (too steep)

# Let's reduce it, recalculate our line and redraw the plot
L50 <- 300
slope <- 0.03
ProbOfMaturity <- 1 / (1 + exp(-slope * (xVals - L50)))

plot(LenCats+25,PropMat,"o",xlab="length",ylab="prop mature",
     frame.plot=F, xlim=c(0,MaxLen),ylim=c(0,1))
arrows(xVals[Positions],PropMat_Low95CL[Positions], 
       xVals[Positions], PropMat_Up95CL[Positions], 
       code=3, angle=90,length=0.02, col='black')
text(LenCats+40,PropMat+0.03,cex=0.6,pos=1,labels=nObsInLenCat)
lines(xVals,ProbOfMaturity, type="o",col="orange")
# getting closer! Not an unreasonable starting point I think!


# Right, so.. how do we fit our curve?

# We know the curve should go through the middle of the data

# We could think about minimising the sum of squared differences
#  (residuals) between observed proportions and expected proportions
#  (derived from our logistic curve). 
# This would give us an answer, but its not the preferred option. 
# Doing this would give "equal weighting" to the data in all length 
#  categories, but as we know from above, we have low sample sizes 
#  in some length categories.
# In addition, when thinking about how certain our results are 
#  likely to be, if we used this approach, we'd have "thrown away" 
#  most of the information we had in our data regarding uncertainty
#  (as data for individual fish would be grouped by category).


# How else to approach it. Fit to data for individual fish...
# As the data are "binary" - i.e. for each fish, there are
#  two possibilities: 
#    the fish is either mature or it is not, 
#    this type of data doesn't lend itself  to calculating "residuals"
#    and minimising the sum of squares, as for the growth analysis

# So we need a new approach - likelihood estimation.


# Let's think about what we're doing a bit more. 
# Let's take our first fish!
MatDat[1,]

# So, it's 263 mm long, and it is recorded as immature (ObsMatCat = 0).
# Would we have expected this fish to be immature, given its size? 
# Based on our logistic curve (given our specified values 
#   for L50 and slope), we can calculate the probability that
#   a fish with this length will be mature
L50 <- 300
slope <- 0.03
LengthOfOurfish <- 263
(ProbOfMaturity_Fish1 <- 1 / (1 + exp(-slope * (LengthOfOurfish - L50))))
# so, on the basis of our curve, we'd say there was a 25% chance that
# a this fish would have been classified as mature, based on what we
# know about its length. We could also have said that the "likelihood"
# of that fish being mature, given its length, is 0.25 (or 25% chance).

# ...but for this fish we recorded that it was immature!
# What is the likelihood that a fish of 263 mm is immature - well, 
#   as there are only two possibilities, mature or not, then the 
#   probability of being immature is 1 - the prob. of being mature.
# Or, we could say, the likelihood of what we observed is 1 - 0.25
Likelihood_Fish1 <- 1 - ProbOfMaturity_Fish1; Likelihood_Fish1


# what about fish 2
MatDat[2,]
# this one has a length of 411 mm, and is recorded as mature.
# how likely is this observation? Because its recorded as mature,
# this is the same as that described by from our curve - prob. of
# being mature at that length.
LengthOfOurfish=411
(ProbOfMaturity_Fish2 <- 1 / (1 + exp(-slope * (LengthOfOurfish - L50))))


# What's the likelihood for all of our data - sum them up? ...Almost. 

# We need one more mathematical trick! As many of our probab. values 
#  are very small (close to zero), if we have lots of them
#  (i.e. lots of data), we can have problems with rounding errors 
#  affecting our results - as computers are only accurate to about 6
#  decimal places.

# We can avoid this problem by working with logarithms.

# So, we calculate the natural logarithms of all our likelihood
#  values (for all our fish), and then sum these up to calculate 
#  an overall log-likelihood.


# 1. Calculate the prob. of each fish being mature, given its length,
# based on our logistic curve
L50 <- 300
slope <- 0.03
ProbOfMaturity <- 1 / (1 + exp(-slope * (MatDat$ObsLen - L50)))

# 2. Calculate the likelihood of each maturity observation
# create an empty vector of the right length to store results
Likelihood <- rep(NA,nobs) 

Likelihood[which(MatDat$ObsMatCat==1)] <- 
            ProbOfMaturity[which(MatDat$ObsMatCat==1)] 
Likelihood[which(MatDat$ObsMatCat==0)] <- 
            1 - ProbOfMaturity[which(MatDat$ObsMatCat==0)] 

# 3. Calculate the natural logarithms for all likelihood values
LL <- log(Likelihood)

# 4. sum them up (or calculate the overall log-likelihood)
(NLL = sum(LL))

# note: when we work with likelihoods, we want to find the 
#    parameter values associated with the "maximum" possible
#    value for the overall likelihood, as this is associated with
#    the best possible fitting curve. 
#    However, if want to use nlminb, it actually wants to find
#   the smallest value for an "objective function" - whether that
#   is be sum of squared residuals used for
#   our growth curve, or in this case, likelihood statistic. 
#   So, what we do in our case is calculate the 
#   overall "negative log-likelihood", 
#   (minimising this is equivalent to maximming the log-likelihod)

# So the calcualtion is:
(NLL = -sum(LL))


# 5. Specify a "function", which is to be passed to the optimizer, 
#  along with the starting values of the parameters. 
# This function needs to calculate, in this case, the probability of fish 
# being mature, given their associated lengths, and the 
# negative log-likelihood (objective function value),
# associated with the data and different sets of maturity parameter values
Calc.NLL.for.maturity <- function(params)
{
  Likelihood <- rep(-999,nobs)
    
 # get expected lengths for each cohort, and overall
 ProbOfMaturity <- 1/(1+exp(-params[2]*(MatDat$ObsLen-params[1])))
  
 Likelihood[which(MatDat$ObsMatCat==1)] <- 
             ProbOfMaturity[which(MatDat$ObsMatCat==1)] 
 Likelihood[which(MatDat$ObsMatCat==0)] <- 
              1 - ProbOfMaturity[which(MatDat$ObsMatCat==0)] 
    
 LL <- log(Likelihood + 1E-4)
    
 # calculate the negative log-likelihood
  NLL = -sum(LL)
    
 # set function result to NLL
 results <- NLL
    
 return(results) 
}

# 6. Specify starting parameters
L50 <- 400
slope <- 0.3

# define starting values of the parameters to be estimated
(params = c(L50,slope))

# checking that the function works and returns a 
#   value for the objective function
test.function <- Calc.NLL.for.maturity(params); test.function


#7. Fit the model

  #7.1 fit model with nlminb optimizer
nlmb <- nlminb(params, Calc.NLL.for.maturity, gradient = NULL, hessian = TRUE)
nlmb
nlmb$par[1]
nlmb$par[2]
nlmb$convergence
# seems OK. Is it really OK?

# We can also use the control and trace arguments to 
#   see what is happening to the likelihood and parameters
#   when nlminb is searching for the parameters associated
#   with the the minimum NLL
nlmb <- nlminb(params, Calc.NLL.for.maturity, 
               gradient = NULL, 
               hessian = TRUE, 
               control=list(trace=1))
nlmb

# hmm, if we look at what's happening to the slope parameter,
#  at one point, it goes negative, which shouldn't happen. 
# The optimisation algorithm seems to have "righted itself",
#  but this isn't ideal, as there could be a problem
#  with other data sets. This time, we were lucky!

# However, we can fix it so that we could never have things
#  go bust, due to the slope parameter being allowed to 
#  go negative. 
# We will use a transformation approach to prevent the slope
#  parameter ever going negative in calculations. 
# As our starting value, we can take the natural logarithm
#  of our intended value. Note that when we back 
#  log-transform a logged value, it can't be negative. 

# So, modify our function for calculating the NLL
Calc.NLL.for.maturity <- function(params)
  {
  
  L50 <- params[1] # this one seems OK, so leave as is.
  slope <- exp(params[2]) # we back log-transform here,
                          # to use in our equation
                          # nlminb is interested in the 
                          # logged value fed into the function,
                          # in the vector params
  
  Likelihood <- rep(-999,nobs)
  
  # get expected lengths for each cohort, and overall
  # we've modifed the equation to use the names of the parameters,
  # for convenience
  ProbOfMaturity <-1/(1+exp(-slope*(MatDat$ObsLen-L50)))
  
  Likelihood[which(MatDat$ObsMatCat==1)] <-
            ProbOfMaturity[which(MatDat$ObsMatCat==1)] 
  Likelihood[which(MatDat$ObsMatCat==0)] <- 
            1 - ProbOfMaturity[which(MatDat$ObsMatCat==0)] 
  
  LL <- log(Likelihood + 1E-4)
  
  # calculate the negative log-likelihood
  NLL = -sum(LL)
  
  # set function result to NLL
  results <- NLL
  
  return(results)
}

# Specify starting parameters, taking the natural 
#   logarithm of the intended startingvalue for the slope
L50 <- 400
lnslope <- log(0.3)

# define starting values of the parameters to be estimated
(params = c(L50,lnslope))

# run nlminb
nlmb <- nlminb(params, Calc.NLL.for.maturity, 
               gradient = NULL, 
               hessian = TRUE, 
               control=list(trace=1))
#Look at the second column, this is the gradient descent,
#i.e. the value of the neg. loglike. at different values of
# the estimable parameters. 
# This should go down.... (we are minimising the neg. loglike)
# Gradient descent is an iterative method. 
# We start with some set of values for our model parameters,
# and improve them slowly.

nlmb

nlmb$par[1] # L50
nlmb$par[2] # ln of slope

# store the parameter estimates from nlminb
Est.L50 = nlmb$par[1]; Est.L50
Est.slope = exp(nlmb$par[2]); Est.slope 
# values are same as before, but this is a slightly 
# more robust way offitting.


# Biologists often like to use a slightly different 
#  parameterisation of  the logistic equation, 
#  containing an "L50" and an "L95", describing the length
#  at which 95% of fish are mature), rather than a slope
#  which is harder to visualise. 
# 
# We can get the L95 value from our original equation using
#  this formula, if we want it
Est.L95 = (log(19)/Est.slope) + Est.L50; Est.L95

# calculate expected maturity at length
(ExpPropMat = 1 /  (1 + exp(-Est.slope * (xVals - Est.L50))))

# or we could do this, which is the same
(ExpPropMat = 1 /  (1 + exp(-log(19)*(xVals - Est.L50) / (Est.L95 - Est.L50))))
# the equations give the same result.

# plot expected curve over observed data
plot(xVals,PropMat,"o",xlab="length",ylab="prop mature",
     frame.plot=F,xlim=c(0,MaxLen),ylim=c(0,1))
arrows(xVals[Positions],PropMat_Low95CL[Positions], 
       xVals[Positions], PropMat_Up95CL[Positions], 
       code=3, angle=90,length=0.02, col='black')
text(LenCats+40,PropMat+0.03,cex=0.6,pos=1,labels=nObsInLenCat)
lines(xVals,ExpPropMat, type="o",col="orange")

# So it fits pretty well throughout most of the range of data, except 
#  for the low end, where there are very few data anyway. 
#  If we were to calcualte confidence limits around the actual curve,
#  they would be broad at particularly the lower end, given 
#  lack of data/information for small fish. 

# We could also plot this way. 
df.bar <- barplot(PropMat,names.arg = LenCats,ylim=c(0,1.2),
                  las=1,cex.axis=0.8,cex=0.8,col="white",
                  ylab="Prop. Mature",xlab="Length, mm")
lines(x = df.bar, y = ExpPropMat,col="orange","o",pch=16)
text(df.bar,PropMat+0.2,cex=0.6,pos=1,labels=nObsInLenCat)


# Now, we need estimates of uncertainty for our estimated 
#  parameter values. 
# Let's get these from optim, another optimisation method,
#  by providing it the estimates of our parameters produced 
#  by nlminb

# set starting values for optim
params <- c(nlmb$par[1],nlmb$par[2])

# run optim, to get parameter estimates, along with the 
#  Hessian matrix, which is required to calculate our 
#  95% confidence limits
opt <- optim(par=params,fn=Calc.NLL.for.maturity,
             method="Nelder-Mead",hessian=TRUE)

# look at parameter estimates from optim and compared with nlminb
opt$par
nlmb$par
# same!

# look at objective function values from optim compared with nlminb
opt$value
nlmb$objective
# same!

# look at convergence criteria
opt$convergence
nlmb$convergence
#OK!

# now we can get the standard errors of the parameter estimates
# from the Hessian matrix, as calculated by optim.

#Standard error of parameters
(std.err <- sqrt(diag(solve(opt$hessian))))

# Let's store all our results - parameter estimates and 95% CL

# first, the "point estimates" 
(Est.L50 <- opt$par[1])
(Est.lnslope <- opt$par[2])
(Est.slope <- exp(opt$par[2]))

# now get approximate 95% confidence limits 
(Est.L50.Low95CL <- Est.L50 - 1.96 * std.err[1])
(Est.L50.Up95CL <- Est.L50 + 1.96 * std.err[1])
(Est.slope.Low95CL <- exp(Est.lnslope - 1.96 * std.err[2]))
(Est.slope.Up95CL <- exp(Est.lnslope + 1.96 * std.err[2]))


# As the L95, together with the L50, are easier
# to visualise than L50 and slope, it may be useful to calculate it.

# However, what's the error for this? Well.., if we want it,
# this requires a bit more work (fit a new curve...).

# The point estimate for L95 is:
(Est.L95 = (log(19)/Est.slope) + Est.L50)

# let's try out this new curve, using the estimated value for L50,
#  and calculated value for L95, based on the L50 and slope 
#  in our original formula, from above
L50 <- Est.L50
L95 <- Est.L95
```

Now calculate the probability of maturity for specified lengths,using this equation:
$$P(mature)=\frac{1}{1+e^{(-log(19)\frac{(Length-L50)}{(L95-L50)})}}$$

```{r}
# Note, using the L50 and L95 with this equation gives the same
#   line as if using the previous maturity equation with L50 
#   and slope (they are mathematically equivalent)
ProbOfMaturity <- 1 / (1 + exp(-log(19) * (xVals - L50) / (L95 - L50)))

# plot it!
  #data
plot(xVals,PropMat,"o",xlab="length",ylab="prop mature",
     frame.plot=F,xlim=c(0,MaxLen),ylim=c(0,1))
arrows(xVals[Positions],PropMat_Low95CL[Positions], 
       xVals[Positions], PropMat_Up95CL[Positions], 
       code=3, angle=90,length=0.02, col='black')
  #prediction
lines(xVals,ProbOfMaturity, type="o",col="blue")

# Let's use this new formula for describing maturity at length,
#  in our function to calculate the negative log-likelihood.

# We'll make the order of our parameters as 
# params[1] = L50
# params[2] = L95
Calc.NLL.for.maturity <- function(params)
{
  
  Likelihood <- rep(-999,nobs)
  
  # get expected lengths for each cohort, and overall
  ProbOfMaturity <-1/(1+exp(-log(19)*(MatDat$ObsLen-params[1])/
                              (params[2]-params[1])))
  
  Likelihood[which(MatDat$ObsMatCat==1)] <-
    ProbOfMaturity[which(MatDat$ObsMatCat==1)] 
  Likelihood[which(MatDat$ObsMatCat==0)] <-
    1 - ProbOfMaturity[which(MatDat$ObsMatCat==0)] 
  
  LL <- log(Likelihood + 1E-4)
  
  # calculate the negative log-likelihood
  NLL = -sum(LL)
  
  # set function result to NLL
  results <- NLL
  
  return(results) 
  
}

# Let's test it out

# define starting values of the parameters to be estimated
(params = c(Est.L50,Est.L95))

# checking that the function works and returns a value for 
# the objective function
test.function <- Calc.NLL.for.maturity(params); test.function
# Yep, OK.

# So, to get the error, we now feed in, from nlminb, our best 
# estimates for L50 and L95 into optim (which has the Hessian matrix)
# required to calculate our 95% confidence limits.
opt <- optim(par=params,fn=Calc.NLL.for.maturity,
             method="Nelder-Mead",hessian=TRUE)

# look at parameter estimates from optim and compared with nlminb
opt$par
(std.err <- sqrt(diag(solve(opt$hessian))))

# Let's store all our results - parameter estimates and 95% CL

# first, the "point estimates" for our growth parameters
(Est.L50 <- opt$par[1])
(Est.L95 <- opt$par[2])

# now get approximate 95% confidence limits for the 
# parameter estimates
(Est.L50.Low95CL <- Est.L50 - 1.96 * std.err[1])
(Est.L50.Up95CL <- Est.L50 + 1.96 * std.err[1])
(Est.L95.Low95CL <- Est.L95 - 1.96 * std.err[2])
(Est.L95.Up95CL <- Est.L95 + 1.96 * std.err[2])

# Great, we have what we want. But you might be wondering, 
#   why didn't we just use the second formula (which the 
#   biologists want), rather that muck around another 
#   equation with the slope parameter?
# Good question! 
# The answer is, the original formula is a bit more "robust".
# When optimising, the algorithm searches for the combination
#  of parameter values that gives the lowest value of, in this case,
#  the negative log-likelihood. 
# When using the second formuala, we get a problem if the L95 value is ever
# equal to or below the L50 value. Let's see what happens
L50 <- 300
L95 <- 250
ProbOfMaturity <- 1 / (1 + exp(-log(19) * (xVals - L50) / (L95 - L50)))
plot(xVals,ProbOfMaturity,"o",xlab="length",ylab="prop mature",frame.plot=F, 
     xlim=c(0,MaxLen),ylim=c(0,1))

# So, if L95 goes below L50, the curves gone the other way - this will 
#  certainly cause problems when optimising! 
# There is never this particular problem when using the formula containing
# the slope and L50. 

# We could use alternative methods for estimating the
# error, such as bootstrapping, which would allow us to estimate also the variation
# around the fitted curve, not just the parameters. see next....
```
## Wrap up exercise
1. Read in "Lesson4_exercise1.csv" 
2. Fit logistic (L50 and L95 version) model using maximum likelihood
3. Calculate 95% confidence limits


<br><br>

# Bootstrapping and Monte Carlo methods

## Bootstrapping
* Bootstrapping is a very simple idea that was conceived by Bradley Efron in the late 1970s.
* It substitudes 'brute force' but it is very computationally demanding
* It works as follows:
  * We have a population of interest which we sample through some sampling process to obtain a random sample of size *n*
  * From the sample we estimate a parameter(s) $\theta$ of interest
  * but...*How good is our estimate of the parameter?*
  * Let's take another sample and estimate $\theta$ again


What bootstrapping does is to assume that our original sample of size *n* is large enough to be representative of the population


So, when bootstrapping, we take the original sample and we resample each observation many times

Each time we draw a sample of size *n*

For each bootstrap sample, we estimate $\theta$, generating a distribution of $\theta$ values that can the be summarised (e.g. mean, SE, etc)

<br>

**Note on time series** 

In a time series, observations depend on previous observations

Thus, we can’t do the ordinary bootstrapping in which observations are randomly drawn from the collection of original observations 

But, we can fit a time series model, and then resample the residuals and add them randomly to fitted values of the time series


```{r}
#1. Simple bootstrapping of turtle size data

# install package
library(boot)

#   1.1 estimate a mean and bootstrap the mean to get 
#       confidence interval

# sizes (lengths of carapace) of hawksbill turtles 
carapace = c(24.0, 25.9, 29.8, 31.0, 31.0, 31.3, 31.5, 32.0, 32.2, 
             33.7, 34.5, 37.5, 37.5, 38.5, 38.8, 40.5, 41.1, 41.4,
             41.7, 43.0, 47.4, 48.2, 49.5, 49.6, 51.5)
summary(carapace)
hist(carapace)

# boot(data, statistic,R)
# data is the data to be resampled, in this case carapace.
#     It can be a vector, matrix or dataframe.
# R is the number of bootstrap samples to take.
# statistic is a function you write to calculate the estimate 
#   from each sample. 

mymean <- function(carapace,i) mean(carapace[i])
# i is a vector created by the boot( ) function containing the indices 
#  of the observations that will be in the bootstrap sample. 
# For example, suppose the data consist of x = c(10,30,40,70). 
#  The vector i passed into the function might be something 
#  like 2,1,3,2 meaning that the bootstrap sample will be composed  
#  of x[2], x[1],x[3], and x[2]


# so mymean() accepts the carapace data and the vector of 
# indices i and then creates a bootstrap sample by extracting the  
# elements of carapace that are indicated in the vector i. 
# (Note: the same observation may be chosen several times)

myboot <- boot(carapace,mymean,R=10000) 
# This says boot( ) will bootstrap carapace by 
#  generating 10,000 samples of carapace (with replacement) 
#  and for each sample it will call mymean and give mymean() 
#  the original data and the instructions (i.e., the indices i)
#  for drawing the sample for computing the mean

#  look at the results
myboot$t[1:5]  # the 10,000 estimates (only showing first 5)
myboot    # The summary of the output from boot.
# Note that "original" refers to the estimate you get when you
# analyze the original data. This is confirmed by:
mean(carapace)

# Note also that the bootstrap standard errors are obtained 
# by calculating the standard error of all the bootstrapped 
# estimates. This is confirmed by:
sqrt(var(myboot$t))               

plot(myboot)    # histogram of bootstrap results and normal Q-Q plot
hist(myboot$t)  # histogram of the bootstrap results

boot.ci(myboot)    # confidence intervals for estimated mean.
# the preferred method is the BCa method which stands for  
# Bias Corrected Accelerated method

boot.ci(myboot,conf=c(0.8,0.9,0.95)) #different percentiles


#   1.2 bootstrapping with multiple variables
# Suppose we are interested in a ratio, such as 
#   gonad weight / total weight, or 
#   catch of fish / fishing effort.
# The data consist of pairs of observations on two variables.
# We note that depending on the circumstance, we may be interested
#  in either sum of x / sum of y (i.e., the ratio of means) or
#  [sum of (x/y)]/n  (i.e., the mean of ratios). 
# We can bootstrap either one.


# Suppose we have pairs of observations on two variables x, and y.
# We put the data in a matrix or dataframe with each pair of 
# observations being a row.
# boot( ) generates a vector i of row indices to resample.

sillydat = matrix(1:20,ncol=2)
sillydat

# here is a function that calculates the mean ratio
sillyfunc1 = function(sillydat,i)
              mean(sillydat[i,1]/sillydat[i,2])

# here is a function that calculates the ratio of means
sillyfunc2 = function(sillydat,i)
              mean(sillydat[i,1])/mean(sillydat[i,2])

# use the functions in boot( ) as follows:
boot(sillydat,sillyfunc1,R=10)
boot(sillydat,sillyfunc2,R=10)


#   1.3 bootstrapping stratified samples
# There are times when we want to impose structure on the 
#  bootstrap resampling.
# For example, in a trawl survey we go to n=30 different 
#  stations and we get a cluster of fish from each station. 
# Now when we do the bootstrapping we
#  can imagine that each station sample represents what's 
#  actually at each station. But, we wouldn't necessarily 
#  want to assume that what was caught at station i might
#  have been caught at station j because there could be real
#  differences among stations. Hence, we might want to 
#  impose structure on the bootstrapping by resampling 
#  each station separately. 

# Here are 102 fish lengths from 9 stations
fishlengths = c(33.9, 28.5, 32.2, 30.1, 37.0, 24.2, 27.8, 
                27.1, 31.8, 28.0, 30.2, 27.3, 31.8, 29.2,
                29.9, 30.7, 24.0, 36.0, 29.6, 29.4, 32.3,
                26.5, 25.5, 27.9, 27.5, 31.0, 29.1, 23.4,
                25.9, 29.1, 27.6, 33.7, 30.4, 30.8, 35.1,
                32.3, 32.6, 32.8, 29.8, 26.4, 28.4, 33.4,
                24.9, 29.8, 27.8, 29.2, 34.8, 29.4, 24.8, 
                34.2, 32.9, 28.3, 35.1, 30.0, 26.9, 27.0,
                29.4, 37.2, 29.0, 35.4, 32.4, 31.6, 33.6,
                31.2, 30.9, 33.2, 26.5, 26.8, 32.7, 31.8,
                26.6, 30.5, 28.3, 27.6, 31.1, 25.8, 30.2,
                29.0, 31.4, 31.5, 31.5, 29.9, 28.8, 31.8,
                33.7, 26.0, 34.0, 30.1, 29.2, 30.5, 29.8,
                25.6, 30.8, 31.3, 29.5, 28.2, 33.1, 31.8,
                25.7,28.6,35.0,27.2)
length(fishlengths)

stations = c(1,1,1,1,2,2,2,2,2,3,3,3,3,3,3,3,3,3,4,4,4,4,4,
             4,4,4,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,6,6,6,6,
             6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,
             6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,7,7,7,7,7,7,7,7,
             8,8,9,9,9,9,9,9,9,9)
length(stations)

# We might be interested in looking if the results differ 
#  by station. 
# Let's look at the mean by station. 
# We can do this using tapply(). 
# The command is of the form: 
#        tapply(vector,subsetting_factor(s),function). 
# Thus, we want to take fishlengths, subset it by stations,
#  and apply mean (or range, or whatever) to each subset.

round(tapply(fishlengths,stations,mean),3)

# let's graph this
means = tapply(fishlengths,stations,mean)
plot(means)   
# (Note: since we gave plot just a vector, it plots the 
#        values versus 1, 2, 3, etc.)

boxplot(fishlengths~stations) 

# We want to bootstrap the mean length by independently resampling
#  the fish within each station, i.e., no fish from station i 
#  can be included in the boostrapped sample for station j. 
# This is accomplished as follows:
myboot=boot(data=fishlengths,statistic=mymean,R=10000,strata=stations)
# note that the station id (= stratum) for each fish is specified  
#  in the argument strata=stations

myboot
mean(fishlengths)
sqrt(var(myboot$t)) 
plot(myboot)    
hist(myboot$t)
boot.ci(myboot)    

```

## Monte Carlo
* Computational simulation that rely on repeated random sampling from a know distribution
* Rather than re-sampling the observations, MC simulations take samples of the estimated parameters
and their corresponding error from a given distribution

```{r}
# 1. Simple example
MC.samp=rnorm(n=10000,mean=5,sd=1)
hist(MC.samp,xlab="",ylab="",col='orange')

# 2. Fit a growth curve to hawksbill turtle mark-recapture data
#   Estimate mean, median and confidence intervals thru
#     Monte Carlo simulations and bootstrapping 

# Read in data
turt <- read.table(paste(hndl,"Lesson4_turtle_data.txt",sep=''),
                   header = T)
head(turt)
dim(turt)   # there are 25 rows of data
# col 1 = size at release (cm),
# col 2 = size at recapture,
# col 3 = time at liberty in yrs

# animals at liberty for at least 1 month (1/12 year)
longterm <- turt[turt$time.lib>(1/12),]
dim(longterm)
time.lib = longterm$time.lib     # rename the variables 
size.rel = longterm$size.rel       
size.recap = longterm$size.recap

observ.inc <- size.recap - size.rel  # observed growth increments
observ.inc < 0          #check if any shrank
sum(observ.inc<0)      # none shrank
sum(observ.inc==0)   #check if any did not grow at all 

# We want to fit a von Bertalanffy growth curve. 
# Fabens (1965) showed that the rate parameter k and the 
#  asymptotic size (called Linfinity) can be estimated from 
#  the size at release, size at recapture and time at liberty 
#  using the method of least squares, i.e the sum of the squared
#  differences between observed size at recapture and the predicted
#  size at recapture is minimized. 

# (Fabens, A.J. 1965. Properties and fitting of the von Bertalanffy 
#  growth curve. Growth 29:265-289.)

#Calculate sums of squares
sumsq <- function(params,size.recap,size.rel,time.lib)
  {
    K <- params[1]
    Linf = params[2]
    
    #Fabens (1965)
    pred.size <- Linf - (Linf - size.rel)*exp(-K*time.lib)
    
    #Sums of squares
    sumofsquares <- sum((size.recap-pred.size)^2)
    
    return(sumofsquares)
}
# We pass in the guesses at k and Linfinity (in params) and 
# also the data. 
# We get back the sum of squares.

# initial guesses for K and Linf
params <- c(.2,185)    

# Now use optim( ) to minimize the sum of squares.
# We pass in:
#   the parameter guesses,
#   the name of the function that calculates the sum of squares
#   and the data

results = optim(params,sumsq,
                 size.recap=size.recap,
                 size.rel=size.rel,
                 time.lib=time.lib,hessian=TRUE)

results
# The parameter estimates are in results$par. 
# The value of the likelihood function calculated 
#   at the parameter estimates is in results$value
# The convergence code of 0 and the message "relative
#   convergence" indicate the procedure appears to have worked well.

# 2.1. Monte Carlo estimation of SE and CI
library(mvtnorm)
set.seed(666)
N.sim=1e3  #number of simulations

VcoV=solve(results$hessian)
system.time({MC=rmvnorm(N.sim,mean=results$par,sigma=VcoV)})
dim(MC)
# number of rows= number of simulations
# number of columns= number of parameters

head(MC)   

#extract quantities of interest
MC_k_mean=mean(MC[,1])
MC_k_median=quantile(MC[,1],0.5)
MC_k_95=quantile(MC[,1],c(.025,.975))

MC_Linf_mean=mean(MC[,2])
MC_Linf_median=quantile(MC[,2],0.5)
MC_Linf_95=quantile(MC[,2],c(.025,.975))


# 2.2 Bootstrapping estimation of SE and CI
turtleboot <- function(turt,i)
{   
  params[1]<- .1
  params[2]<- 85
  ans <-nlminb(params,sumsq2,turt=turt,i=i)
  return(ans$par)
}

# rewrite the function sumsq() in the form wanted by boot(). 
sumsq2 <- function(params,turt,i)
{
  K <- params[1]
  Linf = params[2]
  size.rel <- turt[i,1]
  # note how the vector i is being used to specify 
  # which observations are included in the sample
  
  size.recap <- turt[i,2]
  time.lib <- turt[i,3]
  pred.size <- Linf - (Linf - size.rel)*exp(-K*time.lib)
  sumofsquares <- sum((size.recap-pred.size)^2)
  return(sumofsquares)
}

# run bootstrap
system.time({boot.results=boot(turt[,1:3],turtleboot,R=N.sim)})

# extract quantities of interest
Boot_k_mean=mean(boot.results$t[,1])
Boot_k_median=quantile(boot.results$t[,1],.5)
Boot_k_95=quantile(boot.results$t[,1],c(.025,.975))

Boot_Linf_mean=mean(boot.results$t[,2])
Boot_Linf_median=quantile(boot.results$t[,2],.5)
Boot_Linf_95=quantile(boot.results$t[,2],c(.025,.975))


#Compare Monte Carlo and Bootstrapping
par(mfcol=c(2,2),mar=c(4,4,1,1),mgp=c(2.5,.8,0),cex.lab=1.5)
hist(MC[,1],xlab="K",main="Monte Carlo",xlim=c(0,.4),
     col="orange")
hist(MC[,2],xlab="Linf",main="",breaks=100,col="orange")

hist(boot.results$t[,1],xlab="K",main="Bootstrapping",
     xlim=c(0,.4),col="orange")
hist(boot.results$t[,2],xlab="Linf",main="",breaks=100,
     col="orange")


MC_k_mean;Boot_k_mean
MC_k_median;Boot_k_median
MC_k_95;Boot_k_95

MC_Linf_mean;Boot_Linf_mean
MC_Linf_median;Boot_Linf_median
MC_Linf_95;Boot_Linf_95


#Precision increases with sample size but at a cost....
#For 100000 simulations...
  # Monte Carlo:
# user  system elapsed 
# 0.01    0.00    0.02 

  # Bootstrapping:
# user  system elapsed 
# 289.61    1.16  293.74    #i.e. almost 5 minutes

```
<br>

## Wrap up exercise
1. Read in "Lesson4_exercise3.csv" 
2. Plot **x** VS **y** (*what model can I use?*)
2. Fit non-linear model thru nls and extract mean and 95%CI
3. Fit non-linear model thru maximum likelihood
4. Extract mean and 95% CI thru Monte Carlo simulations
5. Compare mean and 95% CI derived from nls and Monte Carlo 
